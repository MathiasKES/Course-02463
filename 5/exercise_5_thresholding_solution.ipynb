{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning: Simple illustration of the principles\n",
    "\n",
    "In this exercise you will see the benefits of using active learning in a very simple setting. Imagine that we observe some data, $D = \\{d_0, d_1, \\dots, d_i\\}$, on the domain $[0,1]$ with corresponding labels 0 or 1 depending on whether a point is below or above some (unknown) threshold, $\\theta$. Let $x$ denote the value on the domain and $y$ the corresponding label such that $d_i = (x_i,y_i)$ with $x \\in [0,1]$ and $y \\in \\{0,1\\}$. This is also known as the \"high-low\" game and our objective will be to guess the threshold, $\\theta$, by requesting labels for different values of $x$.\n",
    "\n",
    "We shall assume that obtaining a label is \"expensive\" in some sense, hence limiting how many observations for which we can query a label. Our task is to determine which of the available observations we should query, i.e. get a label for, in order for a given learner to improve the most and consequently be able to guess the threshold as quickly as possible.\n",
    "\n",
    "We will use a simple margin classifier which estimates the decision boundary as the threshold with the largest margin to both groups, i.e.,\n",
    "$$\n",
    "    \\hat{\\theta} = \\max(X_0)+\\frac{\\min(X_1)-\\max(X_0)}{2} = \\frac{\\max(X_0) + \\min(X_1)}{2}\n",
    "$$\n",
    "where $X_k = \\{x_i \\mid (x_i,y_i) \\in D, y_i = k \\}$ indicates the set of points with label $k$. In words, we are simply choosing the threshold as the value which evenly divides the version space. Finally, to perform inference on a point, we shall just do a hard assignment of said point as either 0 or 1 depending on whether it is below or above the estimated threshold.\n",
    "\n",
    "This problem is illustrated below with an increasing number of <font color=red>red points</font> and <font color=blue>blue points</font> being observations from the two different classes and the <font color=green>green dashed vertical line</font> is the unknown true threshold. The <font color=#ffe933>yellow dashed line</font> is the estimated threshold of our learner given the data. Notice that in the final drawing the additional data points do not improve the learner. Obviously if you sample infinite points, the learner should improve.\n",
    "\n",
    "How do you classify the following problem setting using the scenario you have learned in the lecture?\n",
    "\n",
    "<span style=\"color:green;font-style:italic\"> \n",
    "As the learner can ask for the label of any point in the 1D input space (synthesize any query), this is actually membership query synthesis.\n",
    "</span>"
   ]
  },
  {
   "attachments": {
    "out.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![out.png](attachment:out.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Noiseless Labels\n",
    "We will start by considering a scenario where there is no noise on the labels, i.e., the labels which we receive from the \"oracle\" are always correct. In this case, the likelihood of the label, $y_i$, is\n",
    "$$\n",
    "    p(d_i | \\theta) =\n",
    "    p(y_i, x_i | \\theta) =\n",
    "    \\begin{cases}\n",
    "            0 & x_i < \\theta \\oplus y_i = 0 \\\\ \n",
    "            1 & \\text{otherwise} \n",
    "    \\end{cases}\n",
    "$$\n",
    "where $\\oplus$ denotes the \"exclusive or\" (`xor`) operator which evaluates to true only when one argument is true and the other is false, i.e., \"when one or the other but not both and not neither\". In this case, $x_i < \\theta \\oplus y_i = 0$ is true when $x_i$ is below the threshold and $y_i=1$ or when $x_i$ is above (or equal) to the threshold and $y_i=0$. These are the cases where the data point is *inconsistent* with the threshold. This simply means that if we know that a given point, $x_i$, has a label of 1, i.e., $d_i = (x_i, 1)$, then we can immediately discard all values of $\\theta$ which are incompatible with this observation. (Please note that whether a point which perfectly coincides with the threshold (i.e., $x_i=\\theta$) is assigned a label of 0 or 1 is arbitrary; we just need to be consistent.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Below you will have to implement three learners each of which uses a different querying strategy.\n",
    "\n",
    "1. `Learner1`. This learner (which really is not very bright!) simply queries points on the whole domain randomly. Strictly speaking, you probably would not consider this a learner as it does not use any of the available data to adjust its behavior.\n",
    "2. `Learner2`. This learner queries points in the *version space* at random. This is better than the first learner in that it actually uses the information that becomes available to it to contrain the interval in which it guesses.\n",
    "3. `Learner3`. This learner also only considers the version space when making queries, however, rather than simply choosing points at random within this space it tries to reduce it maximally each time (on average).\n",
    "\n",
    "In the implementation below, each of these learners are a subclass of the `Learner` class meaning that they have access to all of its attributes and methods. You are required to implement a `query` method for each learner. Take a look at the definition of `Learner` to see how you can access the bounds of the domain and version space which you will need when implementing the different query strategies.\n",
    "\n",
    "Once implemented, you can run an experiment using the `Experiment` class which takes a leaner as input and defines a `run` method which executes the experiment. Code for running the experiment is provided in the file `exercise_5_thresholding_utils.py` and is imported from there. Running an experiment performs the following steps until convergence (which means that the estimated threshold is \"close enough\" to the actual threshold).\n",
    "\n",
    "1. Query learner for candidate point\n",
    "2. Query oracle for corresponding label\n",
    "3. Add data to learner and update version space\n",
    "4. Terminate if converged, otherwise return to step 1\n",
    "\n",
    "You are welcome to look through the code in `exercise_5_thresholding_utils.py` but it is not required to be able to perform the exercise. However, please note that `Oracle` returns `True` or `False` (rather that 1 or 0).\n",
    "\n",
    "* For some of the learners, you will need to sample a random number in an interval. For this purpose you can use `self._rng`, specifically, `self._rng.uniform` which samples from a uniform distribution. `self._rng` is just an instance of `np.random.default_rng` - NumPy's default random number generator object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This defines Experiment, PrababilisticExperiment, and some other stuff\n",
    "from exercise_5_thresholding_utils import *\n",
    "\n",
    "# plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self):\n",
    "        self._rng = np.random.default_rng()\n",
    "        \n",
    "        # lower and upper bounds on the domain\n",
    "        self.domain_lb = 0\n",
    "        self.domain_ub = 1\n",
    "        \n",
    "        # initial bounds on the version space\n",
    "        self.version_space_lb = self.domain_lb\n",
    "        self.version_space_ub = self.domain_ub\n",
    "        \n",
    "        # points which have been queried and their labels\n",
    "        self.x = [] # points\n",
    "        self.y = [] # labels\n",
    "\n",
    "    def add_data(self, x, y):\n",
    "        \"\"\"Add data point and update version space accordingly. This is\n",
    "        used by the Experiment class and you do not need to worry about it\n",
    "        although it might be a good idea to make sure you understand *how* the\n",
    "        version space bounds are updated.\"\"\"\n",
    "        self.x.append(x)\n",
    "        self.y.append(y)\n",
    "        if y:\n",
    "            self.version_space_ub = min(self.version_space_ub, x)\n",
    "        else:\n",
    "            self.version_space_lb = max(self.version_space_lb, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement `Learner1`\n",
    "This learner queries a *random* point on entire domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner1(Learner):\n",
    "    \"\"\"A learner which queries randomly over the entire domain.\"\"\"\n",
    "    \n",
    "    def query(self):\n",
    "        \"\"\"Returns the point (i.e., x) which we would like to have labeled.\"\"\"\n",
    "        # solution::start\n",
    "        return self._rng.uniform(self.domain_lb, self.domain_ub)\n",
    "        # solution::end\n",
    "            \n",
    "\n",
    "# Run an experiment to test the learner\n",
    "learner = Learner1()\n",
    "experiment = Experiment(learner)\n",
    "nsamples = experiment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement `Learner2`\n",
    "This learner also queries a random point, however, only within the version space. Remind yourself what the version space is and check out the `add_data` method of `Learner` which automatically updates the version space each time a new observation is labeled and make sure you understand how the version space is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner2(Learner):\n",
    "    \"\"\"A learner which queries randomly in the version space.\"\"\"\n",
    "    \n",
    "    def query(self):\n",
    "        \"\"\"Returns the point (i.e., x) which we would like to have labeled.\"\"\"\n",
    "        # solution::start\n",
    "        return self._rng.uniform(self.version_space_lb, self.version_space_ub)\n",
    "        # solution::end\n",
    "\n",
    "\n",
    "# Run an experiment to test the learner\n",
    "learner = Learner2()\n",
    "experiment = Experiment(learner)\n",
    "nsamples = experiment.run()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hideOutput": true,
    "tags": []
   },
   "source": [
    "#### Implement `Learner3`\n",
    "The last learner you implemented, was already using the information from the previous samples to reduce the space where we want to search. However, picking new data points at random is not always very efficient. The optimal strategy is one which reduces the version space maximally *on average*. Consider what would be the optimal strategy and implement it. \n",
    "\n",
    "<span style=\"color:green;font-style:italic\">\n",
    "It is quite obvious that we can improve slightly by only selecting points in between the upper and lower bounds like in the previous. However, if we select the point directly in the middle in between the two bound we are guaranteed to reduce the options (version space by half). On average this will be the optimal point to acquire and even though you might get lucky by choosing other points (thus rejecting *more* than half of the space) this is less likely as most often you will just end up cutting a smaller part of the version space.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner3(Learner):\n",
    "    \"\"\"A learner which queries so as to reduce the version space maximally each time.\"\"\"\n",
    "    \n",
    "    def query(self):\n",
    "        \"\"\"Returns the point (i.e., x) which we would like to have labeled.\"\"\"\n",
    "        # solution::start\n",
    "        return (self.version_space_lb + self.version_space_ub) / 2\n",
    "        # solution::end\n",
    "\n",
    "\n",
    "# Run an experiment to test the learner\n",
    "learner = Learner3()\n",
    "experiment = Experiment(learner)\n",
    "nsamples = experiment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run an experiment without printing/plotting anything by using `verbose=False`, e.g., `Experiment(Learner1()).run(verbose=False)`. The `run` method will return the number of samples needed to converge. In particular, we consider a learner to have converged when the size of the version space is smaller than or equal to a selected tolerance level (by default we use 0.01). \n",
    "\n",
    "Using this we can run a lot of experiments (e.g., 10000) with each learner and make a histogram of the number of samples needed for convergence. Is this as you expected? In fact, you will not need to do any simulations for the leaner using the optimal strategy (`Learner3`). Why?\n",
    "\n",
    "<span style=\"color:green;font-style:italic\">\n",
    "`Learner3` is completely deterministic wrt. how much it reduces the version space each time (it is halved). `Learner2` is guaranteed to decrease the version each iteration but with variable amount. This is not true for `Learner1` and its variability is very large compared to `Learner2` since a lot of the times the version space will not even be reduced upon receiving a label.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_experiments = 10000\n",
    "\n",
    "n = {k: [Experiment(learner()).run(verbose=False) for _ in range(n_experiments)] for k, learner in zip(('Learner1', 'Learner2'), (Learner1, Learner2))}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "for ax, k in zip(axes, n):\n",
    "    ax.hist(n[k], 'auto')\n",
    "    ax.set_title(k)\n",
    "    ax.set_xlabel('Samples to converge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy Labels\n",
    "In this section we will extend our analysis to include noisy labels meaning that there is certain risk of a label being wrong. In order to handle this, we will need a probabilistic approach as described below. We start with a uniform prior on the value of the threshold, $p(\\theta)$. Then, using some estimate of which point we believe would be most informative to have labeled, we query the oracle for a label and we update $p(\\theta)$ so as to reflect his information. We repeat this a certain number of times until our \"budget\" is spent and we cannot afford to have any more points labeled.\n",
    "\n",
    "### Updating the Posterior, $p(\\theta | D)$\n",
    "Let $\\epsilon$ denote the probability that a label is wrong, i.e., the probability of the oracle lying (in the code, we call this `p_lie`). To handle this scenario, we will use a probabilistic model where we define the likelihood of observing a particular data point, $d_i=(x_i, y_i)$, as\n",
    "$$\n",
    "    p(d_i  | \\theta) =\n",
    "    p(y_i, x_i | \\theta) =\n",
    "    \\begin{cases}\n",
    "            \\epsilon & x_i < \\theta \\oplus y_i = 0 \\\\ \n",
    "            1-\\epsilon & \\text{otherwise}.\n",
    "    \\end{cases}\n",
    "$$\n",
    "Importantly, and contrary to the noiseless setting, if we know that a given point, $x_i$, has a label of 1 so that $d_i = (x_i, 1)$, then we cannot completely discard all models which are incompatible with this observation since the label could be wrong.\n",
    "\n",
    "Our initial beliefs about the threshold is captured in the prior, $p(\\theta)$, which we shall take to be uniform, i.e., $p(\\theta) \\sim U(0,1)$. Once we have queried the oracle for the label associated with a given point, we would like to update our beliefs about $\\theta$ so as to agree with our observation, i.e., want to compute the posterior probability of $\\theta$. Say we have observed $d_i = (x_i, y_i)$, then we have\n",
    "$$\n",
    "        p(\\theta | d_i) =\\frac{p(d_i | \\theta) p(\\theta)}{p(d_i)}\n",
    "$$\n",
    "where the denominator (the evidence) is computed as\n",
    "$$\n",
    "        p(d_i) = \\int_\\theta p(d_i | \\theta) p(\\theta) d\\theta.\n",
    "$$\n",
    "Once a new observation is added, we repeat the process; now we just use the posterior as the new prior. Consider the addition of the $i$th observation\n",
    "$$\n",
    "        p(\\theta | d_i, D) =\\frac{p(d_i | \\theta, D) p(\\theta | D)}{p(d_i | D)}\n",
    "$$\n",
    "where $D$ contains the previously observed data points.\n",
    "\n",
    "### Calculating the Predictive Probabilities, $p(y | x, D)$\n",
    "To be able to assess how uncertain we are about the label of a particular value of $x_i$, we need to compute the probability of obtaining $(x_i, y_i=0)$ and $(x_i, y_i=1)$ given the current state of our model. In a Bayesian setting, the predictive probability is given by\n",
    "$$\n",
    "    p(y_i | x_i, D) = \\int_\\theta p(y_i | \\theta, x_i) p(\\theta | D) d\\theta\n",
    "$$\n",
    "where $p(\\theta | D)$ is the posterior as calculated above. This computes the probability of a particular observation as the expectation over all possible models (thresholds). This is in contrast to the typical approach of using a point estimate of $\\theta$ (e.g., a maximum likelihood estimate) for prediction as we are effectively integrating out the parameter.\n",
    "\n",
    "As mentioned in the beginning, we perform inference by making hard assignments of the form\n",
    "$$\n",
    "    p(y_i | \\theta, x_i) =\n",
    "    \\begin{cases}\n",
    "            0 & x_i < \\theta \\oplus y_i=0 \\\\ \n",
    "            1 & \\text{otherwise}.\n",
    "    \\end{cases}\n",
    "$$\n",
    "By combining these two expressions, you may be able to convince yourself that $p(y_i=1 | x_i, D)$ is nothing more than the integral of the posterior over all values of $\\theta$ which agree with $(x_i, y_i=1)$. If you find it difficult to interpret the $\\oplus$ operator, here is the above expression written out for the two cases of $y_i=0$ and $y_i=1$\n",
    "\\begin{align}\n",
    "    p(y_i=1 | x_i, \\theta) &=\n",
    "    \\begin{cases}\n",
    "            0 & x_i < \\theta \\\\ \n",
    "            1 & \\text{otherwise}\n",
    "    \\end{cases} \\\\\n",
    "    p(y_i=0 | x_i, \\theta) &=\n",
    "    \\begin{cases}\n",
    "            0 & x_i \\geq \\theta \\\\ \n",
    "            1 & \\text{otherwise}.\n",
    "    \\end{cases}\n",
    "\\end{align}\n",
    "Since we only have two possible labels these are obviously related as\n",
    "\\begin{align}\n",
    "    p(y_i=0 | x_i, \\theta) &= 1-p(y_i=1 | x_i, \\theta).\n",
    "\\end{align}\n",
    "\n",
    "### Selecting the Point to Query for a Label\n",
    "Once we have computed $p(y_i=0 | x_i, \\theta)$ and $p(y_i=1 | x_i, \\theta)$ (the predictive probabilities) for all points of interest we are in a position to evaluate their uncertainty. In the first part we will use least confident and in the second part we will use expected entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the methods `update_posterior` and `compute_pred_prob` of `ProbabisticLearner`. The former updates the posterior and the latter calculates the predictive probabilities according to the formulas given above. You can use the methods `plot_posterior` and `plot_pred_prob` to visually inspect whether your implementation does as you would expect.\n",
    "\n",
    "```\n",
    "# check update_posterior\n",
    "learner = ProbabilisticLearner()\n",
    "fig, ax = plt.subplots()\n",
    "learner.plot_posterior(ax)\n",
    "learner.update_posterior(x=0.3, y=True, p_lie=0.1)\n",
    "learner.plot_posterior(ax)\n",
    "\n",
    "# check compute_pred_prob\n",
    "learner = ProbabilisticLearner()\n",
    "fig, ax = plt.subplots()\n",
    "learner.plot_pred_prob(ax)\n",
    "learner.update_posterior(x=0.3, y=True, p_lie=0.1)\n",
    "learner.plot_pred_prob(ax)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbabilisticLearner:\n",
    "    def __init__(self, p_lie=0, domain_resolution=501):\n",
    "        self.theta = DiscreteDomain([0,1], domain_resolution).points # cryptic way of doing np.linspace(0, 1, domain_resolution)\n",
    "        self.p_theta = np.full_like(self.theta, 1/len(self.theta)) # the (uniform) prior\n",
    "        # The points we have sampled\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "\n",
    "        \n",
    "    def plot_posterior(self, ax=None, color=None):\n",
    "        \"\"\"Plot self.p_theta\"\"\"\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "        return ax.plot(self.theta, self.p_theta, color=color)\n",
    "\n",
    "    \n",
    "    def plot_pred_prob(self, threshold=None):\n",
    "        \"\"\"Plot the result from self.compute_preb_prob().\"\"\"\n",
    "        p_y0, p_y1 = self.compute_pred_prob().T\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(self.theta, p_y0)\n",
    "        ax.plot(self.theta, p_y1)\n",
    "        if threshold:\n",
    "            ax.axvline(threshold, color=\"grey\", linestyle=\"--\")\n",
    "        ax.legend(['p(y=0|x)', 'p(y=1|x)'])\n",
    "        ax.set_xlabel('theta')\n",
    "        ax.set_ylabel('p(y|x)')\n",
    "        ax.set_title('Predictive Probabilities, p(y|x)')\n",
    "\n",
    "        \n",
    "    def plot_uncertainty(self, ax=None, color=None):\n",
    "        \"\"\"Can be used to plot `uncertainty`. Used when running an experiment.\n",
    "        \"\"\"\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "        return ax.plot(self.theta, self.uncertainty, color=color)\n",
    "    \n",
    "\n",
    "    def update_posterior(self, x, y, p_lie, add_data=False):\n",
    "        \"\"\"This methods updates the posterior probabilities for each\n",
    "        value of theta so as to reflect that we have observed the data point\n",
    "        (x,y) at noise level `p_lie`. We use Bayes rule which, in words, states\n",
    "        \n",
    "            posterior = likelihood * prior / evidence\n",
    "\n",
    "        - The prior is our current `p_theta`, i.e., our current beliefs about\n",
    "          the distribution of the threshold.\n",
    "        - The likelihood of observing a particular label given a value of x\n",
    "          and a threshold.\n",
    "        - The evidence just ensures that the posterier integrates (here sums)\n",
    "          to one.\n",
    "\n",
    "        This method should update `self.p_theta` (in place). (It should also\n",
    "        add the data point to `self.x` and `self.y` but this is already\n",
    "        implemented.)\n",
    "        \n",
    "        NOTES\n",
    "        -----\n",
    "        y as returned by the Oracle is either False or True (not 0 or 1)!\n",
    "        \n",
    "        HINTS\n",
    "        -----\n",
    "        * The likelihood depends on the value of x relative to theta and the\n",
    "          value of y and is equal to either p_lie or 1-p_lie. Consider the\n",
    "          following example:\n",
    "              you observe (x=0.3, y=1) at p_lie=0.1. Is it more likely that\n",
    "              the threshold is below 0.3 or above? Specifically, if we know\n",
    "              that the oracle lies with probability 0.1 then we would get the\n",
    "              current data point 9/10 times if the threshold is [above/below]\n",
    "              and 1/10 times if the threshold is [above/below]. If this is\n",
    "              tricky try to make a drawing or consider the \"extreme\" case where\n",
    "              p_lie=0 (as in the previous exercises).\n",
    "        * The purpose of the evidence is simply to ensure that the posterior\n",
    "          normalizes to one, hence this part should be straightforward to\n",
    "          implement.\n",
    "        \"\"\"\n",
    "        if add_data:\n",
    "            # keep track of what data points have been used to update the model\n",
    "            self.x.append(x)\n",
    "            self.y.append(y)\n",
    "\n",
    "        # solution::start\n",
    "        \n",
    "        # get the thresholds that agree with observation (x, y)\n",
    "        # (here we try to be consistent with the notation above)\n",
    "        # we need to handle the case self.theta == x differently depending on\n",
    "        # whether y is True or False.\n",
    "        agree = (self.theta < x) ^ (not y) if y else (self.theta <= x) ^ (not y)\n",
    "        \n",
    "        # compute likelihood * prior\n",
    "        self.p_theta[agree] *= 1-p_lie\n",
    "        self.p_theta[~agree] *= p_lie\n",
    "        \n",
    "        # divide by evidence\n",
    "        self.p_theta /= self.p_theta.sum()\n",
    "\n",
    "        # solution::end\n",
    "        \n",
    "\n",
    "    def compute_pred_prob(self):\n",
    "        \"\"\"This method returns the predictive probabilities, p(y|x), i.e., the\n",
    "        our estimate of the probability of obtaining label 0 or 1 for each\n",
    "        value of x (theta).\n",
    "        \n",
    "        This method should return an (n, 2) array where n = len(self.theta),\n",
    "        i.e., is the number of models/thresholds. The first and second columns\n",
    "        contain the probabilities of y=0 and y=1, respectively.\n",
    "        \n",
    "        HINTS\n",
    "        -----\n",
    "        * To compute this realize that, given a certain threshold, the\n",
    "        probability of assigning a certain label, y, to a particular point, x,\n",
    "        is either 0 or 1 depending on whether the x is above or below the \n",
    "        threshold.\n",
    "        * Note that in the binary case, p(y=0|D) = 1-p(y=1|D).\n",
    "        \"\"\"\n",
    "        # solution::start\n",
    "        p_y1 = self.p_theta.cumsum()\n",
    "        return np.column_stack((1-p_y1, p_y1))\n",
    "        # solution::end\n",
    "\n",
    "\n",
    "    def query_uncertainty(self):\n",
    "        \"\"\"This method returns the most uncertain point, i.e., the point\n",
    "        we wish to request a label for, according to some criterion. It simply\n",
    "        finds the maximum in the `uncertainty` attribute.\n",
    "        \"\"\"\n",
    "        return self.theta[self.uncertainty.argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement `compute_uncertainty` of `LeastConfidentLearner`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeastConfidentLearner(ProbabilisticLearner):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = \"Least Confident\"\n",
    "        \n",
    "    def compute_uncertainty(self):\n",
    "        \"\"\"This method computes the uncertainty of each point according to the\n",
    "        least confident criterion. Please note that the result should be an\n",
    "        *array* of len(self.theta) which is to be stored in `self.uncertainty`.\n",
    "        \n",
    "        When we run an experiment, the desired point is the queried using \n",
    "        `self.query_uncertainty` (already implemented) which simply chooses the\n",
    "        point with the largest value (i.e., the least confident point).\n",
    "        \n",
    "        (The reason we want you to return the uncertainty of each point\n",
    "        explicitly is so that we may plot it when we run an experiment.)\n",
    "        \"\"\"\n",
    "        # solution::start\n",
    "        self.uncertainty = 1-self.compute_pred_prob().max(1)\n",
    "        # solution::end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your results by running an experiment for 20 iterations plotting all steps along the way. Finally, plot the predictive distribution along with the actual threshold. Code for this is provided in the cell below. You can also set `seed` for `ProbabilisticExperiment` which will make your results reproducible in terms of threshold and labels. Using `experiment.run(plot='all')` will run the experiment for 20 iterations and plot each state whereas `experiment.run(n=100)` will run the experiment for 100 iterations displaying only the final state. \n",
    "\n",
    "The markers at the top of the plots show the points which were sampled along with the label which was returned by the oracle (teal = 0, pink = 1). Try to run the experiment a few times and at different noise levels (`p_lie`). How the sampling is affected by wrongly labeled points? Does the model converge to the true threshold?\n",
    "\n",
    "<span style=\"color:green;font-style:italic\">\n",
    "Convergence to the correct values is quite affected by `p_lie` but it seems to at least converge to the correct value after many iterations when `p_lie` is less than 0.45.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = ProbabilisticExperiment(LeastConfidentLearner(), p_lie=0.2, seed=None) \n",
    "experiment.run(plot='all')\n",
    "experiment.learner.plot_pred_prob(threshold=experiment.oracle.threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same but with many more iterations, `n`, and plot only the final result again considering different noise levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = ProbabilisticExperiment(LeastConfidentLearner(), p_lie=0.45, seed=None)\n",
    "experiment.run(n=1000)\n",
    "experiment.learner.plot_pred_prob(threshold=experiment.oracle.threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected Error Reduction\n",
    "<font color=red>This is a more advanced exercise so do the second exercise about uncertainty sampling in the iris dataset (exercise 6) first, then return to this exercise.</font>\n",
    "\n",
    "Here we will extend the treatment above and look at expected error reduction. In particular, we will implement\n",
    "\n",
    "\\begin{align*}\n",
    "    \\phi_\\text{eer}(x) &= -\\sum_{y\\in Y} P_{\\theta}(y | x) \\sum_{x^\\prime \\in U^\\prime} \\phi_\\text{e}(x^\\prime; \\theta^\\prime) \\\\\n",
    "    &= \\sum_{y\\in Y} P_{\\theta}(y | x) \\sum_{x^\\prime \\in U^\\prime} \\sum_{y^\\prime \\in Y} P_{\\theta^\\prime}(y^\\prime|x^\\prime) \\log P_{\\theta^\\prime}(y^\\prime|x^\\prime)\n",
    "\\end{align*}\n",
    "\n",
    "which finds the value that minimizes the *expected entropy* of the predictive probabilities (however, keep in mind that we want to *maximize* the expression above!). $P_\\theta(y | x)$ is the predictive probabilities wrt. the current model (posterior distribution) and $P_{\\theta^\\prime}(y | x)$ refers to the predictive probabilities obtained using a model which have been updated so as to include $(x,y)$. It is important to understand that $P_{\\theta^\\prime}(y | x)$ is the result of adding a *hypothetical* data point, $(x,y)$, i.e., it is the posterior we would get if we were to query the oracle with $x$ and we imagine that it gave us $y$.\n",
    "\n",
    "In the lecture notes, $U$ contains all the points for which we do not have a label, however, here we will simply let $U$ contain *all* possible points (values of $\\theta$) since a label might be wrong. $Y = \\{0,1\\}$ are the possible labels.\n",
    "\n",
    "Try to see if you can parse the above expression and explain in words what we are calculating and what the point, $x^*$, denotes.\n",
    "\n",
    "<span style=\"color:green;font-style:italic\">\n",
    "For each point, $x$, in $U$ (here, all possible values of $\\theta$) we will pretend that we got the sample $(x,0)$, update the posterior, compute the predictive probabilities, and compute its entropy. The entropy is computed inside the parentheses. Next, we do the same pretending that we got the sample $(x,1)$. We repeat this for all points. Finally, we calculate the *expectation* over these entropies (i.e., the cases $(x,0)$ and $(x,1)$) by weighting the entropy by the probability of each label occuring according our *current* model, i.e., $p(\\theta | D)$. Finally, we choose the point which results in the *minimum expected entropy* of the posterior probabilities. This is $x^*$.\n",
    "</span>\n",
    "\n",
    "If you think this looks like is a lot of computation you are completely right! We have to update the model for all combinations of data point and labels which is also why this is seldomly done in practice. It is important to understand that the concept of \"model\" here is simply the posterior of $p(\\theta | D)$ (`p_theta` in the code).\n",
    "\n",
    "Below we define a new learner called `ExpectedEntropyLearner`. Our goal is to implement a method, `compute_expected_entropy`, which finds $x^*$ using the expression above.\n",
    "\n",
    "When computing the entropy you might find that you receive warnings because the probabilities are zero or perhaps even slightly negative (due to machine precision limits). You can use `np.nan_to_num` to handle NaNs returned by the logarithm function, however, you may find that you still get the warnings even though they are being handled by `np.nan_to_num`. You can disable warnings temporarily by for example\n",
    "```\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    # your calculations here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectedEntropyLearner(ProbabilisticLearner):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = \"Expected Entropy\"\n",
    "    \n",
    "    def compute_expected_entropy(self, p_lie):\n",
    "        \"\"\"This method computes the negative expected entropy of the predictive\n",
    "        probabilities and stores the result in `self.uncertainty`. It\n",
    "        should be an array of len(self.theta). \n",
    "        \n",
    "        (Again, the reason we want you to return the uncertainty of each point\n",
    "        explicitly is so that we may plot it when we run an experiment.)\n",
    "        \n",
    "        HINTS\n",
    "        -----\n",
    "        * We need to compute entropy for all combinations of x (possible points\n",
    "          to sample) and y (labels). For each such combination we need to update\n",
    "          the posterior, calculcate the predictive probabilities, and compute\n",
    "          the entropy. At the very end we need to take the expectation over\n",
    "          entropies computed with different labels for each point.\n",
    "        * Before updating the model, store the current state such that the\n",
    "          updates with \"hypothetical\" data point can be easily undone, e.g.,          \n",
    "          \n",
    "            p_theta = self.p_theta.copy()\n",
    "            # do stuff\n",
    "            self.p_theta = p_theta.copy()\n",
    "            \n",
    "        * The expression inside the parenthesis can be computed without any\n",
    "          need to loop explicitly since it is all calculated with the same\n",
    "          model.\n",
    "        (* By default, `add_data=False` of `self.update_posterior`. Leave it\n",
    "          like that such that the hypothetical data points will not be added\n",
    "          when you compute the entropy.)   \n",
    "        \"\"\"\n",
    "        \n",
    "        labels = (False, True)\n",
    "        p_y = self.compute_pred_prob()\n",
    "        H = np.zeros_like(self.compute_pred_prob()) # the entropy values\n",
    "        p_theta = self.p_theta.copy() # to undo the update\n",
    "\n",
    "        # solution::start\n",
    "        def compute_entropy(p):\n",
    "          with warnings.catch_warnings():\n",
    "              warnings.simplefilter(\"ignore\")\n",
    "              return -np.sum(p*np.nan_to_num(np.log2(p)))\n",
    "\n",
    "        for i,x in enumerate(self.theta):\n",
    "            for j,y in enumerate(labels):\n",
    "                # update p(theta) with (x,y) and compute predictive probabilities\n",
    "                self.update_posterior(x, y, p_lie)\n",
    "                p_yij = self.compute_pred_prob()\n",
    "                # compute its entropy. The latter term is not particularly\n",
    "                # important, however, subtracting it removes its influence on\n",
    "                # H[i,j] as we really want to compute the entropy on all data\n",
    "                # points *but* the one used to update the model\n",
    "                H[i,j] = compute_entropy(p_yij) - compute_entropy(p_yij[i])\n",
    "\n",
    "                # undo update\n",
    "                self.p_theta = p_theta.copy()\n",
    "        self.uncertainty = -np.sum(H * p_y, 1)\n",
    "        # solution::end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your results by running an experiment for 20 iterations plotting all steps along the way. Finally, plot the predictive distribution along with the actual threshold. Try to vary `p_lie`.\n",
    "\n",
    "Using the `seed` argument try to compare `LeastConfidentLearner` and `ExpectedEntropyLearner`. Do they seem to agree on the points to sample? How is this affected by the noise level (`p_lie`)? Can you explain this behavior?\n",
    "\n",
    "<span style=\"color:green;font-style:italic\">\n",
    "Yes in most cases uncertainty sampling and expected entropy learners seem to agree, however, particularly when the noise level is high and the density is less peaked evaluating the expected error reduction is different and often lead to more explorative behavior. This is because it directly takes into account how much an unexpected sample might change the model (posterior probability) and hence the predictive probabilities.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low noise level\n",
    "experiment = ProbabilisticExperiment(ExpectedEntropyLearner(), p_lie=0.2, seed=None)\n",
    "experiment.run(plot='all')\n",
    "experiment.learner.plot_pred_prob(threshold=experiment.oracle.threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high noise level\n",
    "experiment = ProbabilisticExperiment(ExpectedEntropyLearner(), p_lie=0.4, seed=None)\n",
    "experiment.run(plot='all')\n",
    "experiment.learner.plot_pred_prob(threshold=experiment.oracle.threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same but with more iterations, `n`, and plot only the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = ProbabilisticExperiment(ExpectedEntropyLearner(), p_lie=0.4, seed=None)\n",
    "experiment.run(n=100)\n",
    "experiment.learner.plot_pred_prob(threshold=experiment.oracle.threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the `data` attribute of `ProbabilisticExperiment` which is a dictionary containing information about the entropy of the posterior (`entropy_posterior`) and the predictive probabilities (`entropy_pred_prob`). It also contains a field `label_wrong` which specifies whether the label obtained from the oracle was correct or not.\n",
    "\n",
    "Here we look at the entropy of the predictive probabilities after adding a new point. How does this compare to the entropy of the posterior and predictive probabilities after sampling the point? Try to produce a plot showing the entropies as a function of samples obtained. How do correct/incorrect labels affect this? Is it consistent with your expectations?\n",
    "\n",
    "<span style=\"color:green;font-style:italic\">\n",
    "The plot below shows the entropy of the posterior and predictive probabilities per iteration. The entropy should decrease over iterations as the model (probability density) becomes more peaked. Note that it is not guaranteed to decrease in this setting for each iteration, in particular, we often see an increase in entropy when the oracle lies and hence produces an unexpected label.\n",
    "</span>\n",
    "\n",
    "You may want to rescale the entropies to plot them together. Perhaps use a logarithmic scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution::start\n",
    "\n",
    "# Rescale the entropies so that they are on a similar scale\n",
    "p_theta = np.array(experiment.data['entropy_posterior'])\n",
    "p_theta /= p_theta.max()\n",
    "p_y = np.array(experiment.data['entropy_pred_prob'])\n",
    "p_y /= p_y.max()\n",
    "lied = np.where(experiment.data['label_wrong'])[0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax.plot(p_theta)\n",
    "ax.plot(p_y)\n",
    "ax.set_ylabel('Entropy (rescaled)')\n",
    "ax.set_xlabel('Sample index')\n",
    "ax.scatter(lied, p_theta[lied], c='r')\n",
    "ax.scatter(lied, p_y[lied], c='r')\n",
    "ax.set_yscale('log')\n",
    "ax.legend(['H[p(theta)]', 'H[p(y|x)]', 'Oracle lied'])\n",
    "ax.grid(alpha=0.25)\n",
    "\n",
    "# solution::end"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "c55ec4b047ebb6fc2c62eaaec3e0c63485962dbc653df72a8e306b5e994c1247"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
