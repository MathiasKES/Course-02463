{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning and Uncertainty Sampling\n",
    "\n",
    "In this exercise we are going to look at uncertainty sampling in the context of classifying flowers from the famous iris dataset. This dataset contains three classes (different species of flowers from the iris family) and four features (measurements of length and width of sepals and petals). This exercise is structured as follows:\n",
    "\n",
    "- First, you will implement three strategies for uncertainty sampling (least confident, margin, entropy). We start by considering only two classes (and, to be able to visualize the results, we will select only two features as well). In this case, all the strategies should agree on the most uncertain point. Subsequently, we will extent this to three classes and see how this affects the uncertainty estimates.\n",
    "- Next, you will implement active learning based on these sampling strategies and compare with random sampling (using all features and classes).\n",
    "\n",
    "The purpose of this exercise is to become familiar with the different strategies for uncertainty sampling and how it can be used in an active learning context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# contains some functions for visualizing the results/progress\n",
    "from exercise_6_iris_utils import *\n",
    "\n",
    "# plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def prepare_data(iris, n_init, use_classes=None, use_features=None, seed=None):\n",
    "    \"\"\"Extract classes and features and split data in training, pool, and test\n",
    "    sets.\n",
    "\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    iris\n",
    "        The iris data object.\n",
    "    use_classes : list | None\n",
    "        The classes to be used. List of class labels (0, 1, 2) or None for all.\n",
    "    use_features : list | None\n",
    "        The features to be used. List of feature indices (0, 1, 2, 3) or None for all.\n",
    "    n_init : int\n",
    "        Number of initial data points in the training set.\n",
    "    seed : int\n",
    "        Seed for reproducibility.\n",
    "    \"\"\"\n",
    "    X = iris['data']\n",
    "    y = iris['target']\n",
    "\n",
    "    # Extract classes and features\n",
    "    if use_classes is not None:\n",
    "        use_examples = np.isin(y, use_classes)\n",
    "        X = X[use_examples]\n",
    "        y = y[use_examples]\n",
    "    if use_features is not None:\n",
    "        X = X[:, use_features]\n",
    "\n",
    "    n = len(X)\n",
    "    assert n_init <= n\n",
    "\n",
    "    # Split in train, pool, and test set\n",
    "    # Use stratified split to make sure we sample all classes equally\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, train_size=n_init / n, random_state=seed)\n",
    "    train, pool = next(sss.split(X, y))\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
    "    pool_, test = next(sss.split(X[pool], y[pool]))\n",
    "\n",
    "    return dict(\n",
    "        train = dict(\n",
    "            X=X[train],\n",
    "            y=y[train]\n",
    "        ),\n",
    "        pool = dict(\n",
    "            X=X[pool[pool_]],\n",
    "            y=y[pool[pool_]]\n",
    "        ),\n",
    "        test = dict(\n",
    "            X=X[pool[test]],\n",
    "            y=y[pool[test]]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and display the included description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "print(iris['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty With Two or Three Classes\n",
    "\n",
    "The purpose of this exercise is to familiarize yourself with the uncertainty sampling strategies described in the notes. As a first step, you will need to implement these strategies. The uncertainty is estimated based on the predictive probabilities from the model, i.e., the probability of each class given a data point, and is a matrix with shape (n_points, n_classes) where each row sums to one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_uncertainty(prob, strategy):\n",
    "    \"\"\"Evaluate the desired uncertainty sampling strategy on predictive\n",
    "    probabilities 'prob'.\n",
    "\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    prob : ndarray \n",
    "        numpy array with predictive probabilities of shape \n",
    "        (n_points, n_classes)\n",
    "    strategy : str\n",
    "        One of 'least confident', 'margin', or 'entropy'.\n",
    "\n",
    "    The function should return an array with uncertainties of shape\n",
    "    (n_points, ) corresponding to the desired strategy.\n",
    "    \"\"\"\n",
    "    # solution::start\n",
    "    if strategy == 'least confident':\n",
    "        res = 1 - prob.max(1)\n",
    "    elif strategy == 'margin':\n",
    "        ix = np.arange(len(prob))\n",
    "        p2, p1 = prob.argsort(1)[:, -2:].T\n",
    "        res = 1 - (prob[ix, p1] - prob[ix, p2])\n",
    "    elif strategy == 'entropy':\n",
    "        res = - np.sum(prob * np.log2(prob), axis=1)\n",
    "    else:\n",
    "        raise ValueError\n",
    "    return res\n",
    "    # solution::end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a function to compute the different uncertainty metrics, we are going to examine them in more detail. Below we are going to\n",
    "\n",
    "- prepare the data,\n",
    "- fit the model on the training data,\n",
    "- compute predictive probabilities, and\n",
    "- compute the uncertainty estimates associated with these probabilities.\n",
    "\n",
    "The predictive probabilities can be evaluated using the `predict_proba` method of the `LogisticRegression` class. Finally, we plot the results (please ignore the colorbar on the top left subplot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "n_init = 20             # Number of points to use for fitting the model\n",
    "use_classes = [0, 1]    # None (all three classes or a list of class labels to use, e.g., [0, 1])\n",
    "use_features = [1, 3]   # The two features to use. Should be in the set (0, 1, 2, 3)\n",
    "seed = 0\n",
    "\n",
    "# Prepare the data (extract and split)\n",
    "data = prepare_data(iris, n_init, use_classes, use_features, seed=seed)\n",
    "\n",
    "# Fit a logistic classifier\n",
    "# (While testing this code I experienced some issues with the default\n",
    "# solver (lbfgs) which is why I use different one here.)\n",
    "model = LogisticRegression(penalty='l2', C=1e1, solver='liblinear', random_state=seed)\n",
    "model = model.fit(data['train']['X'], data['train']['y'])\n",
    "\n",
    "# For the sake of visualization, we are going to create a grid on\n",
    "# which to evaluate probabilities and uncertainties\n",
    "grid, imshow_kwargs = make_grid(data['train']['X'], data['pool']['X'])\n",
    "pool = grid.transpose(1,2,0).reshape(-1, len(use_features))\n",
    "\n",
    "# Evaluate uncertainties on the `pool` variable\n",
    "\n",
    "# solution::start\n",
    "\n",
    "# Compute the predictive probabilities\n",
    "prob = model.predict_proba(pool)\n",
    "\n",
    "# Compute the uncertainties\n",
    "least_confident = evaluate_uncertainty(prob, 'least confident')\n",
    "margin = evaluate_uncertainty(prob, 'margin')\n",
    "entropy = evaluate_uncertainty(prob, 'entropy')\n",
    "\n",
    "# solution::end\n",
    "\n",
    "# Once calculated, plot the uncertainty metrics\n",
    "plot_grid(prob, least_confident, margin, entropy, data, grid, imshow_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, filled circles represent data points used to fit the model. Points are colored according to their *true* label. The bold black cross shows which point each sampling strategy suggests to query next. \n",
    "- (top left) The posterior probabilities plotted as RGB (in the case of two classes, blue is unused).\n",
    "- (top right) Least confidence.\n",
    "- (bottom left) Margin.\n",
    "- (bottom right). Entropy.\n",
    "\n",
    "Go back and do the same using all three classes (`use_classes = None`). You can also try to use different pairs of features.\n",
    "\n",
    "Using three classes, the strategies should no longer agree completely on which point to sample next. Do some of them tend to agree more than others? Why?\n",
    "\n",
    "<span style=\"color:green;font-style:italic\">\n",
    "Margin only considers the two most probable classes at each point and as such will tend to prefer points inbetween two classes irrespective of the total number of classes. On the other hand, entropy considers the full distribution and so its theoretical maximum will be where all (in this case, three) classes are equally probable. The least confident point will also tend to be one in which the probability is close to uniform.\n",
    "</span>\n",
    "\n",
    "Are there other things which might be important to consider besides how uncertain the model is about the label of a given point? For example, your might consider whether these sampling strategies tend to suggest points similiar to those you would have chosen yourself?\n",
    "\n",
    "<span style=\"color:green;font-style:italic\">\n",
    "Considering only uncertainty as a way to assess whether a point is important to sample may lead us to choose points in areas where the uncertainty is high simply because the data density is very low. Consequently, we might be uncertain about points in this region but it may not be particularly important to improve the model here, since we will encounter data in this region only very rarely. Therefore, data density might be another dimension worth considering when deciding which points are relevant to improving model performance.\n",
    "</span>\n",
    "\n",
    "What kind of sampling scenario does this emulate? What scenario are we actually dealing with with this dataset?\n",
    "\n",
    "<span style=\"color:green;font-style:italic\">\n",
    "We are sampling model probabilities on are fairly densely defined grid to emulate a membership query synthesis scenario, i.e., where we can basically query any point we like (although, strictly speaking, this is not so). On the other hand, the iris dataset contains 150 samples and we would need to choose a data point from this set as the next point to include in our model. This is a pool based sampling scenario and is what we will be using below.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning and Random Sampling\n",
    "\n",
    "Up until now we simply extracted a random subset of the data, used this to train the model, and evaluated probabilities and uncertainties. This enabled us to determine which point to sample next, however, we did not actually do this. As such, we have not done any 'active' learning yet. Thus, your task is now to integrate this into an activate learning scenario. Basically, this involves the following steps\n",
    "\n",
    "- Choose some initial data points (training set)\n",
    "1. Fit the model\n",
    "2. Evaluate probabilities on the pool\n",
    "3. Evaluate corresponding uncertainties\n",
    "4. Update training set (and pool) based on the sampled point\n",
    "5. Return to step 1 and repeat for some number of iterations\n",
    "\n",
    "To track the progress you will probably also want to evaluate performance (accuracy) on a test set. This can be done using the `score` method of the model. You should also implement a random sampling approach so that we may compare the two approaches. Finally, plot the classification accuracy of both methods against the number of data available when training the model/number of iterations. One thing to note is that in order to get a stable estimate of the error, you might need to average several runs/model fits so it might be an idea to group these things into functions to easily run them multiple times.\n",
    "\n",
    "For this exercise you should use all classes and all features, however, if you would like to plot the results, you can extract two features and plot the results in the that subspace. You can use the function `plot_pool` to plot the uncertainties for a few iterations (along with the selected point to sample next). (This should be called between step 3 and 4.)\n",
    "\n",
    "> Define a function for fitting the model using either active learning (with one of the uncertainty sampling strategies) or random sampling.\n",
    "\n",
    "Here we provide\n",
    "- a helper function to update the training set and pool and\n",
    "- a skeleton for fitting the model,\n",
    "however, feel free to ignore this and create your own from scratch if you do not find it helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data(data, idx):\n",
    "    \"\"\"Update of the data dictionary from `prepare_data` by moving the data\n",
    "    point with index `idx` from the pool to the training set.\"\"\"\n",
    "    data['train']['X'] = np.append(data['train']['X'], np.atleast_2d(data['pool']['X'][idx]), axis=0)\n",
    "    data['train']['y'] = np.append(data['train']['y'], np.atleast_1d(data['pool']['y'][idx]), axis=0)    \n",
    "    data['pool']['X'] = np.delete(data['pool']['X'], idx, axis=0)\n",
    "    data['pool']['y'] = np.delete(data['pool']['y'], idx, axis=0)\n",
    "\n",
    "def fit_model(iris, paradigm, strategy, n_init, n_iterations, use_classes=None, use_features=None, plot=False):\n",
    "    \"\"\"Run `n_iterations` of active learning or random sampling.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    iris :\n",
    "        The iris dataset object.\n",
    "    paradigm : str\n",
    "        One of 'active learning' or 'random'.\n",
    "    strategy : str\n",
    "        The uncertainty strategy to use (only used when paradigm = \n",
    "        'active learning').\n",
    "    n_init : int\n",
    "        The initial number of points in the training set.\n",
    "    n_iterations : int\n",
    "        The number of iterations to run.\n",
    "    use_classes, use_features (same as for prepare_data)\n",
    "    plot : bool\n",
    "        Whether to plot the state at each iteration (see the\n",
    "        description of plot_pool). It is probably a good idea to\n",
    "        set `n_iterations` low when this is True. You can include\n",
    "        \n",
    "            if plot:\n",
    "                plot_pool(data, idx, uncertainty)\n",
    "        \n",
    "        in between steps 3 and 4. Otherwise, feel free to ignore this\n",
    "        argument.\n",
    "\n",
    "    RETURNS\n",
    "    ----------\n",
    "    scores : ndarray\n",
    "        The score at each iteration.    \n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    data = prepare_data(iris, n_init, use_classes, use_features)\n",
    "    scores = np.zeros(n_iterations)\n",
    "    model = LogisticRegression(penalty='l2', C=1e1, solver='liblinear', warm_start=True)\n",
    "\n",
    "    # Do active learning for `n_iterations` by executing the steps listed above\n",
    "    \n",
    "    # solution::start\n",
    "    for i in range(n_iterations):\n",
    "        # 1. Fit the model\n",
    "        model = model.fit(data['train']['X'], data['train']['y'])\n",
    "\n",
    "        # 2. Evaluate model on pool\n",
    "        prob = model.predict_proba(data['pool']['X'])\n",
    "        scores[i] = model.score(data['test']['X'], data['test']['y'])\n",
    "\n",
    "        # 3. Evaluate uncertainty on pool (or just choose randomly)\n",
    "        if i < n_iterations:\n",
    "            if paradigm == 'active learning':\n",
    "                uncertainty = evaluate_uncertainty(prob, strategy)\n",
    "                if strategy in ('least confident', 'entropy'):\n",
    "                    idx = uncertainty.argmax()\n",
    "                elif strategy == 'maximum margin':\n",
    "                    idx = uncertainty.argmin()\n",
    "                else:\n",
    "                    raise ValueError\n",
    "            elif paradigm == 'random':\n",
    "                uncertainty = None\n",
    "                idx = np.random.choice(np.arange(len(data['pool']['X'])))\n",
    "            else:\n",
    "                raise ValueError\n",
    "            if plot:\n",
    "                plot_pool(data, idx, uncertainty)\n",
    "            # 4. Update training set and pool\n",
    "            update_data(data, idx)\n",
    "    return scores\n",
    "    # solution::end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Run active learning and random sampling for a certain number of iterations. Repeat this a certain number of times, average the result, plot the two results (training set size vs. classification error) and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "n_init = 5\n",
    "n_iterations = 50 - n_init\n",
    "n_avg = 50\n",
    "\n",
    "# Average `n_avg` fits\n",
    "scores_al = np.zeros((n_avg, n_iterations))\n",
    "scores_rn = np.zeros((n_avg, n_iterations))\n",
    "\n",
    "# solution::start\n",
    "for i in range(n_avg):\n",
    "    scores_al[i] = fit_model(iris, 'active learning', 'entropy', n_init, n_iterations)\n",
    "    scores_rn[i] = fit_model(iris, 'random', 'entropy', n_init, n_iterations)\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(np.arange(n_init, n_iterations+n_init), scores_al.mean(0))\n",
    "ax.plot(np.arange(n_init, n_iterations+n_init), scores_rn.mean(0))\n",
    "ax.legend(['active learning', 'random'])\n",
    "ax.set_xlabel('Training set size')\n",
    "ax.set_ylabel('Classification Accuracy')\n",
    "# solution::end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration of the state at each iteration before the next point is added to the training set\n",
    "# (active learning and random sampling)\n",
    "# (Of course this will only work if you implemented the `plot_pool` feature of `fit_model`)\n",
    "n_init = 10\n",
    "n_iterations = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('active learning (entropy)')\n",
    "_ = fit_model(iris, 'active learning', 'entropy', n_init, n_iterations, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('random sampling')\n",
    "_ = fit_model(iris, 'random', 'entropy', n_init, n_iterations, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things you may want to consider.\n",
    "\n",
    "To get a stable estimate of the performance you will probably need to average several runs of active learning and random sampling. What could be the reason (or a reason) for that? You can try to look into the `warm_start` argument of the `LogisticRegression` class and see if that makes a difference.\n",
    "\n",
    "<span style=\"color:green;font-style:italic\">\n",
    "The dataset is quite small so how the data is split probably has a big influence on the initial iterations. Furthermore, initialization of the solver might also matter when very few data points are available to guide the optimization procedure. For example, setting `warm_start = True` (meaning that the model weights from a previous call to `fit` is used as initialization for the subsequent `fit` call) seems to make things quite a bit more stable.\n",
    "</span>\n",
    "\n",
    "Do active learning seem to make a difference for this particular problem? How does settings like number of initial points used and number of queries from the pool affect this?\n",
    "\n",
    "<span style=\"color:green;font-style:italic\">\n",
    "The classification problem is probably a little easy (even though 100 % accuracy cannot be obtained on the whole dataset using a linear classifier) for active learning to really make a difference, however, depending on the number of points in the initial training set (and how many queries are eventually made) active learning seems to outperform random sampling (at least initially, if the initial number of data points is not very large; otherwise a ceiling effect seems to occur. Compare for example `n_init = 5` and `n_init = 40`). Eventually though, with enough queries, we will expect active learning and random sampling to converge to the same solution (however, this might depend on whether the model is (completely) refitted at each iteration or whether the previous step is used as initialization).\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Density Weighting\n",
    "As a final thing, let us have a brief look at how density weighting might affect which point we select. As this data set do not really contain any outliers, we will again be working with a grid of points to illustrate the idea. First, let us set up the experiment (similar to the first exercise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "n_init = 20           # Number of points to use for fitting the model\n",
    "use_classes = None    # None (all three classes or a list of class labels to use, e.g., [0, 1])\n",
    "use_features = [1, 3] # The two features to use. Should be in the set (0, 1, 2, 3)\n",
    "seed = 0\n",
    "\n",
    "# Prepare the data (extract and split)\n",
    "data = prepare_data(iris, n_init, use_classes, use_features, seed=seed)\n",
    "\n",
    "# Fit a logistic classifier\n",
    "# (While testing this code I experienced some issues with the default\n",
    "# solver (lbfgs) which is why I use different one here.)\n",
    "model = LogisticRegression(penalty='l2', C=1e1, solver='liblinear', random_state=seed)\n",
    "model = model.fit(data['train']['X'], data['train']['y'])\n",
    "\n",
    "# For the sake of visualization, we are going to create a grid on\n",
    "# which to evaluate probabilities and uncertainties\n",
    "grid, imshow_kwargs = make_grid(data['train']['X'], data['pool']['X'])\n",
    "pool = grid.transpose(1,2,0).reshape(-1, len(use_features))\n",
    "\n",
    "# Compute the predictive probabilities\n",
    "prob = model.predict_proba(pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the two similarity metrics from the notes (cosine angle and radial basis function, RBF).\n",
    "\n",
    "- Normally, we would calculate the average similarity between all points in the pool, however, for visualization purposes, we have created a grid of points which serves as our \"pool\" (although these points do not exist in the data set). Thus, `compute_similarity` should calculate the average similarity between the grid points (the `pool` variable) and the points in the actual pool (`data[\"pool\"][\"X\"]`) and return a vector of shape `len(pool)`.\n",
    "- To calculate the RBF you can use the `cdist` function from `scipy.spatial.distance` (already imported)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(pool, data, metric):\n",
    "    \"\"\"Return the average similarity of each point in `pool` to the points in\n",
    "    data['pool']['X'].\n",
    "    \n",
    "    pool : (n, d) where n is the number of points on the grid and d is the\n",
    "        number of features.\n",
    "    data : dict containing `pool`.\n",
    "    metric : str\n",
    "        'cosine angle' or 'rbf'.    \n",
    "    \"\"\"\n",
    "    # solution::start\n",
    "    if metric == 'cosine angle':\n",
    "        sim = pool / np.linalg.norm(pool, axis=1, keepdims=True) @ (data['pool']['X'] / np.linalg.norm(data['pool']['X'], axis=1, keepdims=True)).T\n",
    "    elif metric == 'rbf':\n",
    "        sim = np.exp(-cdist(pool, data['pool']['X'])**2 * 1/pool.shape[1])\n",
    "    else:\n",
    "        raise ValueError\n",
    "    return sim.mean(1)\n",
    "    # solution::end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we plot, one of the uncertainty metrics together with the similarity metrics and the combined (i.e., density weighted) metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 1 # the parameter controlling the relative importance of uncertainty and similarity\n",
    "uncertainty = evaluate_uncertainty(prob, 'margin')\n",
    "sim = compute_similarity(pool, data, 'rbf')\n",
    "plot_grid_dw(uncertainty, sim, beta, data, grid, imshow_kwargs)\n",
    "sim = compute_similarity(pool, data, 'cosine angle')\n",
    "plot_grid_dw(uncertainty, sim, beta, data, grid, imshow_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the black x's denote the points in the actual pool, i.e., those we used for calculating similarities, and the colored circles are samples from the different classes.\n",
    "\n",
    "Which of the similarity metrics seem to work best in this case? Does density weighting make sense in this classification setting? Does it depend on the uncertainty metric used?\n",
    "\n",
    "<span style=\"color:green;font-style:italic\">\n",
    "The cosine angle similarity metric does not work particularly well since the mean vector of the pool aligns with the decision boundaries meaning that it does not have much effect (if any). In higher dimensions this is unlikely to happen, though. \n",
    "\n",
    "The radial basis function essentially assumes that the data in the pool is distributed normally. This seems to work better in this case as the density is high around the class in the middle. For least confident and margin this seems to work very well indeed!\n",
    "</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02463",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
